---
title: "Inferencia Causal en R"
output: html_document
date: "2024-06-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Conceptos Iniciales

## Inferencia Causal

Cuando hablamos de un proceso de inferencia causal nos referimos al proceso de hacer afirmaciones sobre una $población$ basándonos en una $muestra$ representativa compuesta de $unidades$  (misma que dependerá de diferentes supuestos), particularmente, usaremos estos dos conceptos para entender el efecto de un fenómeno ocurrido de la $muestra$ sobre la $población$, buscando si es $estadísticamente \ significativo$ para establecer una relación $causal$.

## Modelo de Rubin, Holland 

Una afirmación ya bastante conocida es que correlación no implica causalidad, sin embargo no resulta muy claro el camino a seguir estadísticamente para crear un modelo causal, una parte importante es establecer el concepto de $aleatoreidad$ lo cual obedece a un $proceso \ generador \ de \ datos $ el cual puede se puede aislar  bajo sus propias condiciones. 

Retomando los conceptoa anteriores, consideraremos una $población$, $muestra$ y $unidades$, de las últimas podremos establecer $funcione$ que traduciremos como $variables$. 

En ese sentido, podemos definir los componentes del modelo como: 

* Unidad : Un objeto $i$ en un punto del tiempo específico. 
* Tratamiento : Una intervención $t$, cuyos efectos queremos estimar comparando con un $control / c$ determinado por la ausencia de intervención. 
* Estado del mundo: $S$ para la unidad $i$ la cual tiene un estado de tratamiento definido por $S \in {t,c}$. 
* Resultado potencial: Antes de que el tratamiento es asignado, podemos estimar la medición de nuestro resultado de interés $Y$ para cada unidad $i$, bajo ambos tratamiento y control, identificados como $Y_{i}^{t}$ y $Y_{i}^{c}$.
* Mecanismo de asignación: El algoritmo que determina si cada $i$ recibe $t$ o $c$.
* Efecto causal: La diferencia entre el resultado potencial para una unidad dada: $Y_{i}^{t} - Y_{i}^{c} = T_{i}$. 


Una vez que $S = c$ podemos obtener el $efecto /del tratamiento$ como $Y_{i}^{t} - Y_{i}^{c} = T_{i}$. No obstante una vez que $S$ está determinado solo observamos un resultado, nunca podemos observar dos resultados potenciales para la misma unidad, a esto se refiere el Problema Fundamental de la Inferencia Causal, es decir, es imposible observar el valor de  $Y_{i}^{t}$ y $Y_{i}^{c}$ para la misma unidad y por tanto, es imposible observar el efecto de $t$ contra $c$ para una $i$ en particular.  

// 

Una solución al problema anterior es, en lugar de concentrarnos en calcular $T_{i}$ calculamos $T$ que será el $Efecto / medio / de/  tratamientoo$ calculado como: $T = E(Y_{i}^{t} - Y_{i}^{c}) = E(T_{i}) $ el cual se obtiene directamente recordando que la media es un operador lineal. 

Aunque no siempre podemos observar $T$, podemos estimarlo de la siguiente forma: 

$\hat{T} = \hat{E}(Y_{i}^{t}| S=t) - \hat{E}( Y_{i}^{c}|S=c)$

Generalmente $\hat{T}$ es un buen estimador de $T$ (no quiere decir que sea el único), lo cual depende del mecanismo de asignación.


Consideremos: $E(Y_{i}^{t}) = (Y_{i}^{t}| S=t)$ y $E(Y_{i}^{c}) = (Y_{i}^{c}| S=c)$

Entonces  $E(\hat{T}) = T$ se cumplirá si $S$ es independeinte de $Y$, entonces el grupo tratado es una muestra aleatoria del grupo de los potencialmente tratados, mismo caso para el grupo de control. 

Notemos entonces que la asignación aleatoria de $S$ hace que la independencia sea plausible pero no testeable. 

## Problema de identificación. 

Cuando el supuesto de independencia entre $S$ y $Y$ cuando hay alguna relación a priori entre ambos, por ejemplo, si el tratamiento es parte de un procceso de optimización que involucra al resultado, caso que es común. 

A partir de lo anterior, nos referimos al problema de identificación a la dificultad de obtener un estimador insesgado para identificar parámetros causales a menos que el mecanismo de asignación cumpla con distintas condiciones. 


## Organización y estructura de datos.

### Vectores

Los vectores se usan regularmente para establecer una secuencia numérica, cada elemento del vector revela información sobre los datos en general, se representan comunmente como: $(S_{1}.... S_{6})$, en este caso se tratará de un vector de 6 elementos. 

### Matrices

Las matrices son objectos relacionados en dos dimensiones los cuales crean combinaciones de fila-columna, se representan comunmente como: 

\begin{equation}
\begin{pmatrix}
  a_{a}       & b_{a}   & c_{a}  & d_{a}  \\
  a_{b}      & b_{b}  & c_{b}  & d_{b}  \\
  a_{c}       & b_{c}   & c_{c} & d_{c}  \\
  a_{d}       & b_{d}   & c_{d} & d_{d}  \\
\end{pmatrix}
\end{equation}
=

\begin{equation}
\begin{pmatrix}
  1       & 2   & 3  & 4  \\
  8      & 7  & 6  & 5  \\
  9       & 10   & 11 & 12  \\
  13       & 14   & 15 & 16  \\
\end{pmatrix}
\end{equation}

### Data frame

Un data frame contendrá información sobre múltiples unidades para diferentes vectores, los elemtnos que contienen este data frame regularmente a nivel de fila tendrán identificadores, aunque no siempre puede darse el caso en el que la información esté idenxada, esta última característica será útil si queremos agrupar nuestros datos. 

### Tipos de variables 

* Variables binarias: sólo pueden tomar dos valores de un conjunto discreto. 

* Variables categóricas: capturan si la unidad del dato está clasificada en un rango de categorías. 

* Variables ordinarias: capturan información en un rango particular. 

* Variables continuas: capturan datos en un margen más amplio, generalmente son datos numéricos en un conjunto continuo. 

## Obtención de datos 

* Descargas directas: generalmente son datos con una estructura definida, son de fácil acceso a través de links descargables. 

* Scrapeo de datos: es un proceso de automatización comunmente realizado en R o Python, en el cual se obtienen datos de cada link en una página o bien se retoman usando la estructura HTML de la página en cuestión. 

* Data Capture: son datos que se encuentran previamente clasificados, generalmente siguen una estructura estadística. 


* API: es una fuente en la que podemos acceder más directamente a los datos a través de código, generalmente los datos se encuentran en foramto JSON. 



## Teorema del Límite Central 

* Muestra aleatoria: se refiere a una realización aleatoria de un conjunto de objetos en un data frame, esperamos que al tomar subconjuntos de dicha muestra:

    * La media de ese subconjunto converga (en media) a la             media de la muestra completa.
  
    * La varianza de ese subconjunto sea representativa a la          varianza de nuestra muestra completa. 
    
* Lo anterior se cumplirá sí y sólo sí los datos son independientes e idénticamente distribuidos (no hay ninguna conexión entre las observaciones o filas para un dataframe), es decir, si no hay subgrupos en la población que podrían tener resultados correlacionados. 

* Si quisíeramos hacer algún tipo de inferencia partiendo de los grupos de un data frame tenemos que usar el método de aleatorización estratificada para asegurarnos de considerar submuestras representativas para cada subgrupo. 


## Distribuciones: nos muetran el comportamiento las varibales aleatorias, y son particularmente útiles para realizar pruebas de hipótesis. Podemos distinguir entre: 

  * Discretas: Involucran descontinuidades escalonadas en la        densidad, también tienen la naturaleza de mostrar realizaciones binarias, es por eso que los ejemplos más comunes son las distribuciones Bernoulli y Binomial. 
  * Continuas: Tienen una forma suavizada, son un caso de una        distribución discreta, la forma más común es la distribución normal (Gaussiana)
  
* Lo anterior nos sierve para establecer el Teorema del Límite Central, el cual establece que conforme el número de realizaciones aumenta, la media muestral tiene a distribuirse de forma normal alrederor la media de la población, también la desviación estandar tiende a disminuir conforme $n$ disminuye (a causa de la convergencia en media). 

* Para una realización aleatoria podemos hacer inferencias sobre qué podría ser representativo de nuestra población, conforme agregamos más datos, nuestra inferencia se vuelve más robusta. 
  


```{r, out.width="50%"}

knitr::include_graphics("Sample_subset.png")

```

## Analítica Descriptiva 





## Hipótesis Alternativa/ Hipótesis Nula

Vamos a concentrarnos en dos conceptos importantes: 

* $H_0$ : Hipoótesis Nula refiriéndose a que el efecto no es $estádisticamente \ significativo$ o bien que no hay efecto.
* $H_a$ : Hipótesis Alternativa, refiriéndose a que el efecto  es $estádisticamente \ significativo$.



Generalmente buscamos desacreditar la hipótesis nula como objetivo de la investigación.


## 



```{r cars}

library(ggplot2)
library(NHANES)
library(tidyverse)

colnames(NHANES)

# Create bar plot for Home Ownership by Gender
ggplot(NHANES, aes(x = Gender, fill = HomeOwn)) + 
  # Set the position to fill
  geom_bar(position = "fill") +
  ylab("Relative frequencies")

# Density plot of SleepHrsNight colored by SleepTrouble
ggplot(NHANES, aes(x = SleepHrsNight, color = SleepTrouble)) + 
  # Adjust by 2
  geom_density(adjust = 2) + 
  # Facet by HealthGen
  facet_wrap(~ HealthGen)


#Using the NHANES dataset, let's investigate the relationship between gender and home ownership. Remember, more information #about the dataset can be found here: NHANES.

# Subset the data: homes
homes <- NHANES %>%
  # Select Gender and HomeOwn
  select(Gender, HomeOwn) %>%
  # Filter for HomeOwn equal to "Own" or "Rent"
  filter(HomeOwn %in% c("Own", "Rent"))
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
