---
title: "Inferencia Causal en R"
output: html_document
date: "2024-06-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Conceptos Iniciales

## Inferencia Causal

Cuando hablamos de un proceso de inferencia causal nos referimos al proceso de hacer afirmaciones sobre una $población$ basándonos en una $muestra$ representativa compuesta de $unidades$  (misma que dependerá de diferentes supuestos), particularmente, usaremos estos dos conceptos para entender el efecto de un fenómeno ocurrido de la $muestra$ sobre la $población$, buscando si es $estadísticamente \ significativo$ para establecer una relación $causal$.

## Modelo de Rubin, Holland 

Una afirmación ya bastante conocida es que correlación no implica causalidad, sin embargo no resulta muy claro el camino a seguir estadísticamente para crear un modelo causal, una parte importante es establecer el concepto de $aleatoreidad$ lo cual obedece a un $proceso \ generador \ de \ datos $ el cual puede se puede aislar  bajo sus propias condiciones. 

Retomando los conceptoa anteriores, consideraremos una $población$, $muestra$ y $unidades$, de las últimas podremos establecer $funcione$ que traduciremos como $variables$. 

En ese sentido, podemos definir los componentes del modelo como: 

* Unidad : Un objeto $i$ en un punto del tiempo específico. 
* Tratamiento : Una intervención $t$, cuyos efectos queremos estimar comparando con un $control / c$ determinado por la ausencia de intervención. 
* Estado del mundo: $S$ para la unidad $i$ la cual tiene un estado de tratamiento definido por $S \in {t,c}$. 
* Resultado potencial: Antes de que el tratamiento es asignado, podemos estimar la medición de nuestro resultado de interés $Y$ para cada unidad $i$, bajo ambos tratamiento y control, identificados como $Y_{i}^{t}$ y $Y_{i}^{c}$.
* Mecanismo de asignación: El algoritmo que determina si cada $i$ recibe $t$ o $c$.
* Efecto causal: La diferencia entre el resultado potencial para una unidad dada: $Y_{i}^{t} - Y_{i}^{c} = T_{i}$. 


Una vez que $S = c$ podemos obtener el $efecto /del tratamiento$ como $Y_{i}^{t} - Y_{i}^{c} = T_{i}$. No obstante una vez que $S$ está determinado solo observamos un resultado, nunca podemos observar dos resultados potenciales para la misma unidad, a esto se refiere el Problema Fundamental de la Inferencia Causal, es decir, es imposible observar el valor de  $Y_{i}^{t}$ y $Y_{i}^{c}$ para la misma unidad y por tanto, es imposible observar el efecto de $t$ contra $c$ para una $i$ en particular.  

// 

Una solución al problema anterior es, en lugar de concentrarnos en calcular $T_{i}$ calculamos $T$ que será el $Efecto / medio / de/  tratamientoo$ calculado como: $T = E(Y_{i}^{t} - Y_{i}^{c}) = E(T_{i}) $ el cual se obtiene directamente recordando que la media es un operador lineal. 

Aunque no siempre podemos observar $T$, podemos estimarlo de la siguiente forma: 

$\hat{T} = \hat{E}(Y_{i}^{t}| S=t) - \hat{E}( Y_{i}^{c}|S=c)$

Generalmente $\hat{T}$ es un buen estimador de $T$ (no quiere decir que sea el único), lo cual depende del mecanismo de asignación.


Consideremos: $E(Y_{i}^{t}) = (Y_{i}^{t}| S=t)$ y $E(Y_{i}^{c}) = (Y_{i}^{c}| S=c)$

Entonces  $E(\hat{T}) = T$ se cumplirá si $S$ es independeinte de $Y$, entonces el grupo tratado es una muestra aleatoria del grupo de los potencialmente tratados, mismo caso para el grupo de control. 

Notemos entonces que la asignación aleatoria de $S$ hace que la independencia sea plausible pero no testeable. 

## Problema de identificación. 

Cuando el supuesto de independencia entre $S$ y $Y$ cuando hay alguna relación a priori entre ambos, por ejemplo, si el tratamiento es parte de un procceso de optimización que involucra al resultado, caso que es común. 

A partir de lo anterior, nos referimos al problema de identificación a la dificultad de obtener un estimador insesgado para identificar parámetros causales a menos que el mecanismo de asignación cumpla con distintas condiciones. 


## Organización y estructura de datos.

### Vectores

Los vectores se usan regularmente para establecer una secuencia numérica, cada elemento del vector revela información sobre los datos en general, se representan comunmente como: $(S_{1}.... S_{6})$, en este caso se tratará de un vector de 6 elementos. 

### Matrices

Las matrices son objectos relacionados en dos dimensiones los cuales crean combinaciones de fila-columna, se representan comunmente como: 

\begin{equation}
\begin{pmatrix}
  a_{a}       & b_{a}   & c_{a}  & d_{a}  \\
  a_{b}      & b_{b}  & c_{b}  & d_{b}  \\
  a_{c}       & b_{c}   & c_{c} & d_{c}  \\
  a_{d}       & b_{d}   & c_{d} & d_{d}  \\
\end{pmatrix}
\end{equation}
=

\begin{equation}
\begin{pmatrix}
  1       & 2   & 3  & 4  \\
  8      & 7  & 6  & 5  \\
  9       & 10   & 11 & 12  \\
  13       & 14   & 15 & 16  \\
\end{pmatrix}
\end{equation}

### Data frame

Un data frame contendrá información sobre múltiples unidades para diferentes vectores, los elemtnos que contienen este data frame regularmente a nivel de fila tendrán identificadores, aunque no siempre puede darse el caso en el que la información esté idenxada, esta última característica será útil si queremos agrupar nuestros datos. 

### Tipos de variables 

* Variables binarias: sólo pueden tomar dos valores de un conjunto discreto. 

* Variables categóricas: capturan si la unidad del dato está clasificada en un rango de categorías. 

* Variables ordinarias: capturan información en un rango particular. 

* Variables continuas: capturan datos en un margen más amplio, generalmente son datos numéricos en un conjunto continuo. 

## Obtención de datos 

* Descargas directas: generalmente son datos con una estructura definida, son de fácil acceso a través de links descargables. 

* Scrapeo de datos: es un proceso de automatización comunmente realizado en R o Python, en el cual se obtienen datos de cada link en una página o bien se retoman usando la estructura HTML de la página en cuestión. 

* Data Capture: son datos que se encuentran previamente clasificados, generalmente siguen una estructura estadística. 


* API: es una fuente en la que podemos acceder más directamente a los datos a través de código, generalmente los datos se encuentran en foramto JSON. 



## Teorema del Límite Central 

* Muestra aleatoria: se refiere a una realización aleatoria de un conjunto de objetos en un data frame, esperamos que al tomar subconjuntos de dicha muestra:

    * La media de ese subconjunto converga (en media) a la             media de la muestra completa.
  
    * La varianza de ese subconjunto sea representativa a la          varianza de nuestra muestra completa. 
    
* Lo anterior se cumplirá sí y sólo sí los datos son independientes e idénticamente distribuidos (no hay ninguna conexión entre las observaciones o filas para un dataframe), es decir, si no hay subgrupos en la población que podrían tener resultados correlacionados. 

* Si quisíeramos hacer algún tipo de inferencia partiendo de los grupos de un data frame tenemos que usar el método de aleatorización estratificada para asegurarnos de considerar submuestras representativas para cada subgrupo. 


## Distribuciones: nos muetran el comportamiento las varibales aleatorias, y son particularmente útiles para realizar pruebas de hipótesis. Podemos distinguir entre: 

  * Discretas: Involucran descontinuidades escalonadas en la        densidad, también tienen la naturaleza de mostrar realizaciones binarias, es por eso que los ejemplos más comunes son las distribuciones Bernoulli y Binomial. 
  * Continuas: Tienen una forma suavizada, son un caso de una        distribución discreta, la forma más común es la distribución normal (Gaussiana)
  
* Lo anterior nos sierve para establecer el Teorema del Límite Central, el cual establece que conforme el número de realizaciones aumenta, la media muestral tiene a distribuirse de forma normal alrederor la media de la población, también la desviación estandar tiende a disminuir conforme $n$ disminuye (a causa de la convergencia en media). 

* Para una realización aleatoria podemos hacer inferencias sobre qué podría ser representativo de nuestra población, conforme agregamos más datos, nuestra inferencia se vuelve más robusta. 
  


```{r, out.width="50%"}

knitr::include_graphics("Sample_subset.png")

```

## Analítica Descriptiva: Medidas de dispersión y localización

### Medidas de localización/tendencia central

Consideremos que la localización central de un data frame (para un vector) requiere considerar diferentes medidas de tendencia central como: 

* Media: También llamada Esperanza o Valor Esperado y denotado comunmente como $\bar{X}$ es igual a la suma del vector dividido entre su longitud (notemos que este valor indirectamente que el peso de cada valor en el vector es el mismo en términos de probabilidad). 

* Mediana: El valor que se encuentra justo en medio del vector listado (el número de valores previos y posteriores es el mismo aproximádamente). 

* Moda: El valor con más ocurrencia en el vector. 

### Medidas de dispersión

Generalmente los datos están distribuidos alrededor de una tendencia central, para organizarlos mejor distinguimos entre: 

* Rango: Diferencia entre el valor más pequeño y el más grande en una secuencia numérica (vector numérico).

* Percentiles/Quartiles: Muestran al densidad de la distribución sobre un punto en particular. 

* Rango IQ: Distancia entre dos segmentos de datos. 

* Varianza: Suma de los datos estandarizados (restándoles la media) divivido entre $n-1$, es decir: $\sigma^2 = 1/n-1 \sum_{1}^n (X_{i} - \bar{X})^2$ recordemos que la Desviación estándar es la raíz cuadrada de la Varianza (i.e $\sigma$).


### Correlación

* La correlación nos ayuda a evaluar la relación entre dos variables ($X$, $Y$; o más), la visualización más común es el gráfico de dispersión donde podemos observar como cuando un vector aumenta su valor, los valores del otro vector aumentan (relación directa), disminuyen (relación inversa) o se quedan igual (no hay relación), podemos obtener la correlación calculando $\sum_{1}^n (X_{i} - \bar{X})(Y_{i} - \bar{y})/ SD^X SD^Y$, esta medida deriva de la covarianza y es importante mencionar que la correlación no implica causalidad. 

## Visualizaciones

* Tablas 

* Boxplots

* Histogramas 

* Densidad 


## Randomized Control Trials (RCTs)

Partiendo del modelo de Rubin, podemos recordar que si el tratamiento es determinado de forma aleatoria, entonces la independencia en la muestra es plausible (mas no garantizada), los RCTs hacen uso de este supuesto para generar modelos de asignación aleatoria. Algo importante a diferencias son los tratamientos exógenos vs los endógenos.

* La idea detrás es que cuando los agentes eligen el estado del mundo $S$ (endógeno) dependederá en los resultados $Y$.

* Por otro lado, si es exógeno podemos definir un "agente optimizador fuera del control" que hace la independencia plausible. 

Lo anterior se puede entender también en un problema de exogeneidad sobre una regesión continua, lo cual le resta robustez y significancia estadística a nuestro modelo y por tanto a la intepretación causal de los resultados, cuando hacemos un RCT la idea principal es resolver el problema de endogenidad teniendo una muestra independiente a priori, aunque también podemos encontrarnos con el problema de una aleatorización pobre, una forma de verificar que no ocurre dicho problema es realizando balance tests, los cuales buscan verificar la independencia del estado del mundo $S$ con las características exógenas (no el resultado $Y$).


## Limitantes de los RCT

### Beneficios:

* Dado que podemos establecer independencia a priori, el problema de identificación se resuelve fácilmente, pudiendo establecer un efecto causal. 

### Limitantes o problemas potenciales:

* Aleatorización fallida 
* Desgaste (Attrition)
* Efectos en equilibrio general
* Practicidad 
* Validación externa
* Sesgo por elección


Attrition: Algunos participantes pueden dejar el estudio, el problema está en que no podremos encontrar $Y$ de las personas que abandonan el estudio, si el attrition no está correlacionado $Y^{s}$, no será un problema, dado que aún se podrá tener independencia de $S$ y $Y^{t}$ y $Y^{c}$ en el resto de la muestra, si el attrition está correlacionado con $Y^{S}$ y el tratamiento cambia el attrition entonces la indepedencia se rompe, necesitaríamos: 

$E[Y^{S}|S = s, observada] = E[Y^{S}, observada]$

Observaríamos un conjunto no aleatorio de los resultados tratados. 


## Compliance y la intención a ser tratado (ITT)

En algunas ocasiones también las personas se pueden negar a tomar el tratamiento, también puede darse el casp que las personas del grupo de control busquen tratarse fuera del programa, si es que se observan los resultados, se puede obtener un estimador válido del parámetro sobre la intención a ser tratado (ITT), podemos entender mejor lo anterior a través de la siguiente Matrix de Compliance: 

|   |      Assignación t      |  Assignación c |
|----------|:-------------:|------:|
| Recibo t |  A | B |
| Recibo c |    C   | D |


En este caso, A y D son compliers, mientras que B y C no lo serán. De inicio, podríamos pensar que nuna buena opción sería la de comparar A y B contra C y D, pero se perdiría la independencia dado que tenemos agentes endógenos que eligen en el estado del tratamiento, esto porque la asignación de $t$ o $c$ no necesariamente implica que hayan recibido $t$ o $c$ necesariamente. 

De tal forma se desarrolla el concepto de la intención a ser tratado, el cual compara A y C así como B y D, si todos los agentes son compliers entonces se cumplirá que ITT = ATT.


## Efectos en Equilibrio General

Ocurre cuando el grupo de control es afectado por el tratamiento, puede darse debido a efectos en equilibrio general o bien a efectos de pares (peer effects), en caso que se identifiquen esos efectos de forma potencial, el RCT debe de ser rediseñado para prevenirles, lo cual regularmente reduce el tamaño de la muestra efectiva a través de clusters.


## Practicidad y validación. 

* Ética: Hay experimentos que pueden generar estrés financiero o una situación poco deseable para la muestra. 

* Factibilidad: Existe una limitante sobre los recursos que podemos usar y el escenario que podemos plantear. 

* Costo: Los RCT's tienden a ser costosos en comparación con los estudios observacionales. 

* Validación externa: Una estimador será válido internamente si es un estimador insesgado para la población del experimento, también será válido externamente si es un estimador insesgado para la población fuera del experimento, no todos los RCT tienen validación externa pero se busca que la tengan. 

Los RCT's serán nuestro marco estándar para estimar una inferencia causal a causa de que la independencia es plausible al controlar aleatoriamente el mecanismo de asignación, resolviendo el problema de identificación, por eso cuando examinamos datos observacionales (no experimentales) siepre es buena idea comparar con el RCT inicial para comprender los problemas potenciales, aunque es importante tener en mente las limitantes de los RCTs, teniendo claro el tipo de estimación que queremos obtener, así como los parámetros que son creíbles para estimar, considerando attrition, compliance, equilibrio general, etc.

### Regresión Bivariada 

También llamada OLS(Ordinary Least Squares) Bivariada o Mínimos Cuadrados Ordinarios Bivariado, nos ofrece un modelo en el que se ajusta una línea entre dos variables X y Y, por lo que se asume de inició una relación lineal entre X y Y tal que: 

\begin{equation}
Y = \beta_{0} + \beta_{1}X + u
\end{equation}

MCO (OLS) será el método para estimar la pendiente $\beta_{1}$ y el intercepto $\beta_{0}$, aunque también se podrá estimar a través de máxima verosimilitud. 

La ecuación estándar de regresión para una población será: 

\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}X_{i} + u_{i}
\end{equation}

Donde:

* $Y_{i}$ es la variable dependiente.
* $X_{i}$ es la variable independiente. 
* $\beta_{0}$ es la constante/intersecto verdadera.
* $\beta_{1}$ es la pendiente verdadera. 
* $u_{i}$ es el término de error verdadero.


Por otro lado, nuestros estimadores serán marcados con un "sombrero" de la siguiente forma: 

* $\hat{\beta}$ será el estimador del coeficiente. 
* $\hat{Y_{i}}$ será el valor predecido.

Por lo que $\hat{Y} = \hat{\beta_{0}} + \hat{\beta_{1}} X_{i}$

* $\hat{u_{i}}$ es un residuo. 

Por lo que $ \hat{u_{i}} = Y_{i} - \hat{Y_{i}} = Y_{i} - \hat{\beta_{0}} - \hat{\beta_{1}} X_{i}$

Observamos $X$ y $Y$ y estimamos $\beta$ y $u$.


## Mínimos Cuadrados Ordinarios (MCO)

Sigue una lógica de optimización en la que se minimizan las desviaciones de los valores predecidos con los valores reales en los datos, generalmente toma una muestra de tamaño $n$ dado por: 


\begin{equation}

min_{b_{o}b_{1}} = \sum_{i}^{n}(Y_{i} - b_{0} - b_{1}X_{i})^2

\end{equation}

Los residuos se refieren a los errores que el modelo está haciendo, los cuales están dados por $\bar{u_{i}} = Y_{i} - \hat{\beta_{0}} - \hat{\beta_{1}} X_{i}$, podemos reescribir el problema de MCO como: 


\begin{equation}
min_{b_{o}b_{1}} \sum_{i=1}^{n} (\tilde{u_{i}})^2
\end{equation}

donde $\tilde{u_{i}} = Y_{i} - b_{0} - b_{1}X_{i}$, la solución dada por MCO minimiza la suma de los residuos al cuadrado, es decir mnimiza la distancia vertical entre los puntos. 


```{r, out.width="50%"}

knitr::include_graphics("OLS.png")

```

Notemos sin embargo que $\hat{\beta_{0}}$ y $\hat{\beta_{1}}$ son los valroes de $\beta_{0}$ y $\beta_{1}$ que minimizan la suma de residuos al cuadrado, donde cada uno está dado por: 


\begin{equation}

\hat{\beta_{1}} = \frac{\sum_{i=1}^{n}(X_{i} - \bar{X}) (Y_{i} - \bar{Y})} {{\sum}_{i=1}^{n} (X_{i} - \bar{X})^2}
\end{equation}

\begin{equation}
 =  \frac{\hat{Cov(X,Y)}}{\hat{Var(X)}} = \frac{S_{XY}}{S_{X}^2}
\end{equation}



\begin{equation}
\hat{\beta_{0}} = \bar{Y} - \hat{\beta_{1}}\bar{X}
\end{equation}

Donde 

$\hat{X} = \frac{1}{n} \sum_{i=1}^{n} X_{i}$ es la media muestral de X.

y $\hat{Y} = \frac{1}{n} \sum_{i=1}^{n} Y_{i}$ es la media muestral de Y.

El método de MCO nos ayuda a encontrar $\hat{\beta_{s}}$, lo cual define el valor predecido y los residuos. 

Los valores predecidos ($\hat{Y}$ es el modelo estimado de $Y_{i}$ dado $X_{i}$): 

\begin{equation}
\hat{Y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}X_{i}
\end{equation}

Predice los valores a partir de los estimadores de MCO usando medias condicionales: 

\begin{equation}
E(Y_{i} | X_{i}) = \beta_{0} + \beta_{1}X_{i}
\end{equation}

\begin{equation}
\hat{E}(Y_{i} | X_{i}) = \hat{\beta_{0}} + \hat{\beta_{1}}X_{i} = \hat{Y_{i}}
\end{equation}

Los valores predecidos son las mejores aproximaciones de $Y_{i}$ cuando sabemos $X_{i}$.

Los residuos serán la diferencia entre los resultados veraderos y los valores predicidos. 

\begin{equation}
\hat{u_{i}} = Y_{i} - \hat{Y_{i}} = Y_{i} - \hat{\beta_{0}} - \hat{\beta_{1}}X_{i} 
\end{equation}


## Supuestos importantes para MCO - Teorema de Gauss - Markov 

MCO será un "buen" método para estimar una relación causal entre $X$ y $Y$ a través del estimador/pendiente $\hat{\beta_{1}}$, se necesitará que: 

1. Los parámetros verdaderos sean causales. 
2. MCO deberá proporcionar un buen "estimador" de los valores reales. 

Recordemos que $\hat{\beta_{0}}$ y $\hat{\beta_{1}}$ son variables aleatorias, no constantes, por lo que un "buen" estimador deberá de ser insesgado y preciso/eficiente. 

Pensemos que queremos estimar a través de MCO la siguiente relación lineal entre $X$ y $Y$: 

\begin{equation}
 Y_{i} = \beta_{0} + \beta_{1}X_{i} + u_{i}, i = 1,...,n 
\end{equation}


MCO será el mejor estimador bajo los siguientes supuestos para todo $i$: 

1. $E(u_{i}|X_{i}) = 0 $ para todos los valores de $X_{i}$.
2. $(X_{i}, Y_{i}$ son independientes e identicamente distribuidas.
3. Los outliers son poco comunes (curtosis finita).
4. $var(u_{i}|X_{i}) = \sigma_{u}^2$ para todos los valores de $X_{i}$ (homocedasticidad).


## Supuesto 1: $E(u_{i}|X_{i}) = 0$ 

Media condicional igual a cero, se refiere a que la tendencia central de la distribución del término de error $u_{i}$ no está relacionado a $X_{i}$, es decir la media de $u$ es la misma para cualquier valor de $X$. 

Notemos que $E(u_{i})=0$ no es relevante acá, nos importa $E(u_{i}|X_{i})$ (esto dado que la media condicional es el mejor predictor lineal en general) sea la misma para todos los valores de $X$ i.e, $E(u_{i}) = E(u_{i}|X_{i})$, aunque notemos que $E(u_{i} = )$ mientras que la regresión tenga una constante 

El supuesto de la media condicional igual a cero es más fuerte que el supuesto de no correlación, es decir: 

* $E(u|X) = 0 \implies corr(u,X) = 0 $ pero 

* $corr(u,X) = 0  \notimplies E(u|X) = 0 \newcommand{\notimplies}{\;\not\!\!\!\implies}$

Notemos que el hecho de que nuestro témino de error $u$ y $X$ estén no correlacionados es una condición necesaria pero no suficiente para que el supuesto 1 se cumpla, podemos ilustrar esto de la siguiente forma: 

\begin{equation}
 wage = \beta_{0} + \beta_{1}educ + u
\end{equation}

Necesitamos considerar los factores que serán capturados por $u . E(u|educ) = o$ implica por ejemplo que $E(u|trunca$ = $E(u|grduado$. Esto debido a que existen elemntos que no se capturan con educación en nuestro modelo inicial y serán capturados por $u$, por lo que $u$ podría ser la ambición, inteligencia, condiciones de mercado laboral locales, salud, etc. 
Cualquier cosa que determine $Y$ y no esté en $X$ estará en nuestro "término de error", incluyendo las lurking variables, las cuales serán variables que pueden aparentar tener un nexo causal fuerte de forma errónea. 

Si $X$ está correlacionado con el término de error, el supuesto 1 será violado, y sí $X$ es asigndo aleatoriamente, entonces no esperaíamos que el término de error esté correlacionado con $X$, en un RCT el tratamiento es asignado aleatoriamente por lo que el supuesto 1 es plausible. 

Por tanto, si el supuesto 1 se cumple entonces esperaríamos que: 

1. Los residuos $\hat{u}$ siempre estarán no correlacionados a $X$. 

2. Los residuos $\hat{u}$ siempre tendrán media cero. 

3. Si corremos la regresión $\hat{u} = \delta_{0} + \delta_{1}X + \epsilon$ entonces obtenemos $\hat{\delta_{0}} = \hat{\delta_{1}} = 0 $ siempre. 

4. Los residuos no están correlacionados con $X$ por construcción. 

5. Nos importa si el error está correacionado o no correlacionado con $X$.

Por tanto, no podremos testear si $E(u|X) = 0$ checando si $E(\hat{u}|X) =0 $ dado que esto último siempre es cierto, tampoco podríamos si se cumple $E(u|X) = 0$ directamente dado que nunca observamos $u$, por lo que tendremos que decidir si el supuesto 1 es plausible dada la información que sabemos de los datos (por lo que resulta importante pensar en el mecanismo de asignación), otra opción sería observar las relaciones no lineales entre $\hat{u}$ y $X$. 


## Supuesto 2: $(X_{i},Y_{}i)$ son independientes e identicamente distribuidas.

Consideremos que cada unidad de los datos proviene de una realización de la misma población y cada realización es independiente entre sí, notemos que los datos panel generalmente violan esa condición, las series de tiempo también violan ese supuesto regularmente. 


## Supuesto 3: Los outliers son "poco comunes". 

Es decir $X$ y $Y$ tienen curtosis finita, haciendo referencia al cuarto momento central i.e : $ 0 < E(X^4) < \infty$, esta condición también nos sirve para asegurar que tienen una varianza y covarianza finita y bien definida. 

## Insesgadez de MCO 

Consideremos el siguiente modelo especificado para la población: 

\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}X_{i} + u_{i} 
\end{equation}

Si los supuestos 1-3 se cumplen entonces tendremos: 


\begin{equation}
  E(\hat{\beta_{0}}) = E(\beta_{0}) 
\end{equation}

\begin{equation}
  E(\hat{\beta_{1}}) = E(\beta) 
\end{equation}


Donde \hat{\beta_{0}} y \hat{\beta_{1}} son estimadores del MCO, lo cual será cierto sin importar la muestra $n$. 

## Aproximaciones a la normal (teoría asintótica)

Cuando tenemos una $n$ lo suficientemente grande, la distribución de $\hat{\beta}$ se aproxima de forma cercana a la normal, i.e: 


\begin{equation}
  \hat{\beta_{1}} N(\beta_{1}, \sigma_{\hat{\beta_{1}}}^2)
\end{equation}

\begin{equation}
  \sigma_{\hat{\beta_{1}}}^2 = \frac{1}{n} \frac{var [( X - \mu_{X})u ]}{ var(X)^2}
\end{equation}

Donde $\mu_{x}$ es la media de la población $X$. 

En este caso usaremos una aplicación del teorema del límite central y nos ayudará a hacer pruebas de hipótesis de forma analítica. 

Generalmente se sugiere nuestra muestra $n >100$ sera suficientemente grande, aunque esto depende en las características de los datos, en algunos casos incluso muestras grandes no serán suficientes dado que no existe suficiente independencia entre los puntos.


## Supuesto 4: Homoscedasticidad. 

Notemos que aunque es posible $E(u|X)=0$ pero $var(u|x)$ puede variar en $X$, si la varianza de de $u$ es diferente para diferentes niveles de $X$, diremos que el término de error es heterocedastico, aunque notemos que si el término de error es homoscedastico entonces nuestros estimadores de MCO serán mejores pero la homoscedasticidad no se requiere para que nuestro MCO sea insesgado 


## Mejor Estimador Lineal (MEL)

Cuando los supuestos 1-4 se cumplen entonces MCO será el mejor estimador lineal, el cual será 1. el más eficiente/mejor, 2. Lineal (en contraposición con modelos no lineales o de máxima verosimilitud), 3. Insesgado condicionalmente y 4. Estimador. 


## Variaciones en los estimadores MCO 

Recordemos que $\hat{\beta_{1}}$ es una variable aleatoria, por lo que tendrá una media y si los supuestos 1-3 se cumplen, entonces será insesgado, i.e: 

\begin{equation}
E(\hat{\beta_{1}}) = \beta_{1}
\end{equation}

Y tiene una desviación estándar de $\sigma_{\hat{\beta{1}}}$: 

\begin{equation}
  \sigma_{\hat{\beta_{1}}}^2 = \frac{1}{n} \frac{var [( X - \mu_{X})u ]}{ var(X)^2}
\end{equation}

Donde $n$ es una muestra, el denominador es la varianza de $X$ al cuadrado.

Sin embargo, es importante preguntarnos: ¿qué determina la precisión de los estimadores MCO? 

Pensemos en el caso homoscedastico: 


\begin{equation}
  \sigma_{\hat{\beta_{1}}}^2 = \frac{1}{n} \frac{var [( X - \mu_{X})u ]}{ var(X)^2}
\end{equation}

\begin{equation}
         = \frac{1}{n} \frac{var(X) var(u)]}{ var(X)^2}
\end{equation}

\begin{equation}
         = \frac{1}{n} \frac{var(u)}{ var(X)^2} = \frac{1}{n} \frac{\sigma_{u}^2}{\sigma_{X}^2} 
\end{equation}

Es decir que tendremos:

\begin{equation}
         = \frac{1}{n} \frac{\sigma_{u}^2}{\sigma_{X}^2} 
\end{equation}

\begin{equation}
        = \frac{1}{\sqrt{n}} \frac{\sigma_{u}}{\sigma_{X}} 
\end{equation}

La precisión de (\frac{1}{var}) del estimador MCO es mayor mientras: 

* El tamaño de la muestra $n$ aumente. 
* La varianza de $u$ disminuya.
* La varianza de $X_{i}$ aumente.

La intuición será que tendremos que observar $Y$ para un rango más grande de $X$ para obtener más información. 



```{r, out.width="75%"}

knitr::include_graphics("Precision_OLS.png")

```

En el gráfico podemos ver que si bien en ambos casos muestran la verdadera $\beta$, habrá más varianza en $X$ del lado derecho, sin embargo ¿cómo podemos saber cuál caso será más preciso? una manera de hacerlo es a través de simulaciones, en tal caso podríamos usar una simulación Monte Carlo como en el siguiente ejemplo:


```{r, out.width="65%"}

knitr::include_graphics("MonteCarlo.png")

```

Podemos ver que al momento de hacer una simulación Monte Carlo con la misma $\beta$ real, mismo $n$ y mismo $\sigma_{u}$ pero con mayor varianza en $X$ del lado derecho será más preciso dado que obtenemos más información sobre la distribución de $\hat{\beta_{1}}$

## Medición de ajuste : R cuadrada. 

Recordemos que tenemos: 
\begin{equation}
Y_{i} = \hat{Y_{i}} + \hat{u_{i}}
\end{equation}

De lo cual obtenderemos la suma total de los cuadrados: TSS$= \sum_{i=1}^n(Y_{i} - \bar{Y})^2$, TSS será el valor total de la cantidad de variación en la muestra, notemos que $var(Y_{i}) = var(\hat{Y_{i}}) + var(\hat{u_{i}})$. 

Por otro lado, una vez que tenemos $\hat{Y_{i}}$ estimado, entonces podemos obtener la suma de cuadrados explicados dada por ESS $= \sum_{i=1}^n(\hat{Y_{i}} - \bar{Y})^2$ lo cual será la cantidad de variación explicada por $X$.

Así mismo, tendremos la suma de los residuos cuadrados dado por SSR $\sum_{i=1}^n \hat{U}^2$ que será la cantidad de variación no explicada por $X$, notemos que TSS = ESS+SSR. 

A partir de lo anterior podemos definir la R cuadrada $R^2$ la cual se define como la fracción de la varianza muestral en $Y_{i}$ que está explicado por $X_{i}$, es decir:

\begin{equation}
R^2 = \frac{ESS}{TSS}
\end{equation}

\begin{equation}
R^2 = 1 - \frac{SSR}{TSS}
\end{equation}

Donde la $R^2$ oscila entre 0 y 1, por lo que: 

* $R^2 = 1 \iff SSR=0$
* $R^2 = 0 \iff SSR=0$

Por lo que una cantidad más cercana a 1 un mejor ajuste de nuestro modelo o bien una proporción más grande de la variación explicada. 


## Medición de ajuste : Error estándar de la regresión

El error estándar de la regresión es una medida del error de predicción y usado para estimar la varianza de los coeficientes, un estimador de la desviación estándar de $u_{i}$ es el error estándar de la regresión (SER) dado como: 


\begin{equation}
SER = \frac{1}{n-2} \sum_{i=1}^n \hat{u_{i}^2} = \frac{1}{n-2} SSR
\end{equation}

El SER mide la típica desviación entre $Y_{i}$ y $\hat{Y_{i}}$ y $n-2$ será los níveles de libertad (tamaño de la muestra menos el número de parámetros estimados).


```{r, out.width="55%"}

knitr::include_graphics("Fitted.png")

```
Notemos que los datos del lado derecho son los mismos que los del lado izquierdo con el error reducido (con varianza menor de $u$), por lo que la gráfica del lado derecho tendrá una mayor $R^2$ dado que una fracción de la variación en $Y$ es explicada por $X$. 



## MCO : Inferencia - Hipótesis Alternativa/ Hipótesis Nula

Vamos a concentrarnos en dos conceptos importantes: 

* $H_0$ : Hipoótesis Nula refiriéndose a que el efecto no es $estádisticamente \ significativo$ o bien que no hay efecto.
* $H_A$ : Hipótesis Alternativa, refiriéndose a que el efecto  es $estádisticamente \ significativo$. 

Generalmente buscamos desacreditar la hipótesis nula como objetivo de la investigación y hacer inferencia estadística sobre hipótesis relativo a los parámetros de la población $\beta$, de tal forma podemos decir que: 

* $H_0: \beta_{1} = \theta$ vs $H_{A}: \beta_{1} \neq \theta$ (two-sided)

* $H_0: \beta_{1} = \theta$ vs $H_{A}: \beta_{1} < \theta$ (one-sided)

Generalmente, se usan los test two-sided de forma más apropiada. 

## Intervalos de confianza

Imaginemos que tenemos $\hat{\beta} = 0.084$ y el error estándar de $\hat{\beta}$ (SE($\beta$)) = 0.014. 

Con la información anterior podríamos construir elintervalo de confianza a 95%a para $\beta$ (usando una aproximación normal) como: $[\hat{\beta}-1.96 *SE(\hat{\beta}), \hat{\beta}+1.96*SE(\hat{\beta})] = [0.0567, 0.114]$ la primera parte nos dará la cola final y la segunda parte nos dará la cola inicial.

Lo anterior se puede traducir también como que el 95% de las muestras posibles, el Intervalo de Confianza (CI) incluirá el valor verdadero. 

Para cualquier $\theta$ dentro del Intervalo de Confianza (CI) se incluirá el valor verdadero, para cualquier $\theta$ dentro del CI no se rechazará $H_{0} : \beta =0 $ a un nivel del 5%. 

## Estadístico t

Será ampliamente utilizado, el test default en Stata será contra cero (no efecto) efecto nulo, el estadístico t de $H_{0}: \beta_{1} = 0 $, tomando el ejemplo anterior: $\frac{0.084 -0}{0.014} = 6.0 $

Pero podemos hacer el mismo test contra cualquier otra hipótesis nula, el estadístico t de $H_{0}: \beta_{1} = 0.1 $, tomando el ejemplo anterior: $\frac{0.084 -0.1}{0.014} = -1.14 $

Podemos recordar que si W es una variable normal, si le restamos la media y dividimos sobre la desviación estándar nos lleva a una variable normal estándar, es decir: 

\begin{equation}
W - N(\mu_{W}, \sigma_{W}^2) \rightarrow \frac{W - \mu_{W}}{\sigma_{W}} - N(0,1)
\end{equation}

Si la desviación estándar es estimada en lugar de estar dada, entonces al momento de estandariar nos llevará a una distribución t

\begin{equation}
W - N(\mu_{W}, \sigma_{W}^2) \rightarrow \frac{W - \mu_{W}}{\hat{\sigma_{W}}} - t_{n-k-1}
\end{equation}

Y cuando n-k-1 es grande, entonces $t$ es aproximadamente normal. 


```{r, out.width="55%"}
knitr::include_graphics("Est_t.png")
```
```{r, out.width="55%"}
knitr::include_graphics("Est_t2.png")
```

```{r, out.width="55%"}
knitr::include_graphics("Est_t3.png")
```

```{r, out.width="55%"}
knitr::include_graphics("Est_t4.png")
```

Podemos interpretar los grados de libertad, pero ¿cuándo será n-k-1 suficientemente grande? podemos ver en el gráfico anterior cuándo la distribución t empata con la distribución normal, mientras que cuando aumentamos los grados de libertad de 1 a 5, 10 y 30 se acerca más a la distribución normal. 

Así como el MCO será nuestro estimador estándar del regresor, el $t-stat$ será nuestro test estadístico estándar. 

Sobre la hipótesis nula de que $\beta$ es igual a alguna constante $\theta$ ($H_{0}: \beta = \theta$) podemos formular el estadístico t:

\begin{equation}
\frac{\hat{\beta - \theta}}{\hat{\sigma_{\hat{\beta}}}} -.- t_{n-k-1}
\end{equation}

Si el estadístico t es muy pequeño, no podremos rechazar la hipótesis nula. 

También existe una "regla del pulgar" la cual implica que  $|t-stat| >2$ implica que podemos rechazar la hipósteis nula.

```{r, out.width="45%"}
knitr::include_graphics("Est_t5.png")
```

No rechazamos la hipótesis nula $H_{o}$ si $|t-stat|<2$ dado que el valor absoluto de ese estadístico t es menor que 2. 


```{r, out.width="50%"}
knitr::include_graphics("Est_t6.png")
```

En este caso sí rechazamos la hipótesis nula $H_{o}$ si $|t-stat|>2$ dado que el valor absoluto de ese estadístico t es mayor que 2. 

Los resultados anteriores se pueden observar en la siguiente tabla, misma que generalmente se muestra después de correr un modelo estadístico: 


```{r, out.width="50%"}
knitr::include_graphics("Tabla_est.png")
```


Podemos observar como la constante será 70.3973 lo cual significa que en esta muestra en particular tendremos individuos que son aproximadamente 70.39 inch de altura cuando female (mujer) = 0, ¿qué pasa cuando female =1? utilizaremos el valor de -10.322 menos cero, la hipótesis nula en ese sentido sería que no existe una diferencia entre las alturas de hombres y mujeres en la muestra, y será dividio sobre el error estándar (SE) dado por 1.699


Stata automaticamente reporta $H_{0}: \beta_{1} = 0$, en el caso que hemos estado discutiendo sería:

\begin{equation}
\frac{-10.33 -0}{1.699} = -6.08
\end{equation}

Recordemos que el test nos muestra si el estimador del coeficiente es "estadísticamente significativo diferente de cero", dado que tomaremos el valor absoluto, por lo que que sea negativo no impacta realmente, notemos que es mayor a 2 por lo que podemos refutar fácilmente la hipótesis nula en este caso, este test también sugiere que el coeficiente estimado es estadísticamente significativo, dado que será diferente de 0 donde 0 es nuestra hipótesis nula, generalmente se toman estos resultados como muestras de patrones descriptivos que pueden sugerir causalidad.  

## Valores $p$

Los valores $p$ se definen como la probabilidad de obtener una $\hat{\beta}$ que sea lo suficientemente lejana al valor nulo como del resultado observado, si el nulo es verdadero entonces definimos: 

\begin{equation}
p - value \aprox 2\Phi(-|t|)
\end{equation}

El resultado anterior usa una aproximación de la normal estándar para la distribución t con diferentes niveles de libertad. 

Parecido al estadístico t, lo anterior depende del valor nulo en cuestión: 

* $p$-value (para $(H_{0}: \beta = 0) \aprox 2\Phi (-6.08) < 0.00001$)

* $p$-value (para $(H_{0}: \beta = -9) \aprox 2\Phi (0.782) = 0.439$)

## MCO: Causalidad 

Si bien es algo común encontrar correlacoones entre diferentes variables, no quiere decir que tods esas correlaciones nos lleven a causalidad, supongamos que hay una relación lineal causal: $X$ causa a $Y$. 

Si estimamos por MCO $ X = \beta_{0} + \beta_{1}Y$ podemos generar una $\beta_{1}$ diferente de cero y tiene una interpretación inversa. 

Incluso si tuviésmos la dirección de la causadlidad de forma correcta, MCO podría danos un estimador sesgado para $\beta_{1}$ en la mayoría de los casos. 

Bajo circunstancias especiales MCO nos puede dar un etimador "bueno" de la relación causal entre $X$ y $Y$. 

Cuando los datos se generan de un RCT realizado correctamente, al momento de aplicar MCO obtendremos un estimador insesgado para una relación causal dado que de inicio tenemos correctamente identificado nuestro nexo causal y también se satisfacen las condiciones del Teorema de Gauss-Markov. 

Podemos pensar en el caso en el que tenemos un tratamiento bivariado, los valores predecidos (esperanas condicionadas estimada) si $X$ es una variable binaria se definen de la siguiente forma: 

Definamos una variable dummy como =1 si el enunciado es verdadero, 0 en otro caso. Por ejemlo, una variable dummy de tratamiento será 1 si $i$ es tratado y o en otro caso (si $i$ forma parte del grupo de control). 

\begin{equation}
\hat{E}(Y_{i}|X_{i} = 1) = \hat{\beta_{0}} + \hat{\beta_{1}} x 1 = \hat{\beta_{0}} + \hat{\beta_{1}}

\end{equation}


\begin{equation}
\hat{E}(Y_{i}|X_{i} = 0) = \hat{\beta_{0}} + \hat{\beta_{1}} x 0 = \hat{\beta_{0}} 
\end{equation}

$\beta_{0}$ será la media del control cuando $X=0$
$\beta_{1}$ será la diferencia en medias del grupo de control cuando $X=1$ y $X=0$

## Sesgo por variable omitida, introducción a MCO multivariado

Podeemos pensar en cuáles son los retornos de la educación, es decir, cuánto aumenta el salario de un individuo por un año adicional de educacion. 

Supongamos que tenemos datos sobre los salarios y los años de educación de una muestra aleatoria de trabajadores, donde corremos la siguiente regresión: 

\begin{equation}
 log(wage_{i}) = \beta_{0} + \beta_{1}educ_{i} + u_{i}
\end{equation}

¿Será $\hat{\beta_{1}}$ un estimador insesgado de $\beta_{1}$, el retorno a la educación? (Es decir: ¿se cumplirá que $E(\hat{\beta_{1}}) = \beta_{1}$ ?)

Consideremos que hay otros factores que determinan los salarios, por ejemplo los años de experiencia, por tal motivo, podemos pensar en un modelo más verdadero para determinar los salarios como: 

\begin{equation}
log(wage_{i}) = \beta_{0} + \beta_{1}educ_{i} + \beta_{2}exp_{i} + v_{i}
\end{equation}

Asumamos que los supuestos MCO se cumplen, por lo que $E(\hat{\beta_{1}}) = \beta_{1}$ para esta regresión multivariada. 

Pero podemos omitir la experiencia y estimaríamos:

\begin{equation}
log(wage_{i}) = \beta_{0} + \beta_{1}'educ_{i} + u_{i}
\end{equation}

Pensemos que un modelo MCO bivariado sería incompleto, con un término de error dado por $v = \beta_{2}exp + u$

\begin{equation}
log(wages_{i}) = \beta_{0} + \beta_{1}educ_{i} + \beta_{2}exp_{i} + u_{i}
\end{equation}


Pensemos en el modelo incmpleto con un término de error dado por $v = \beta_{2}exp + u$, por lo que: 


\begin{equation}
log(wages_{i}) = \beta_{0} + \beta_{1}'educ_{i} + \beta_{2}exp_{i} + v_{i} ...(1)
\end{equation}  

Con


\begin{equation}
v_{i} = \beta_{2}exp_{i} + u_{i}
\end{equation} 

Entonces: 

\begin{equation}
log(wages_{i}) = \beta_{0} + \beta_{1}'educ_{i} + v_{i} ...(2)
\end{equation}  

Supongamos que estimamos la ecuación (2), recordemos el supuesto de la media condicional igual a cero, por lo que $E(v|educ) = 0$ para todos los valores de $education$, por lo que el término de error debería de ser el mismo para todos los niveles de $education$, por lo que la experiencia debería de ser la misma para todos los niveles de $education$, lo cual no parece ser del todo razonable. 

Recordemos que el estimador de $\beta_{1}$

\begin{equation}
\hat{\beta_{1}'} = \frac{\hat{Cov(X,Y)}}{\hat{Var(X)}}
\end{equation}

Desarrollando el numerador vemos que: 

\begin{equation}
\hat{Cov(educ, log(wages))} = \hat{Cov(educ, \beta_{1}educ + \beta_{2}exp + u)}
\end{equation}

\begin{equation}
= \beta_{1}\hat{Var(educ)} + \beta_{2}\hat{Cov(educ,exp)} + \hat{Cov(educ,u)}
\end{equation}

Dado que suponemos que $E(u|educ) = 0 $, por lo que $\hat{Cov(educ,u)} = 0$

\begin{equation}
= \beta_{1}\hat{Var(educ)} + \beta_{2}\hat{Cov(educ,exp)} 
\end{equation}

Por lo que el sesgo de variable omitidas será: 

\begin{equation}
\hat{\beta_{1}}'= \beta_{1} + \beta_{2}  \frac{\hat{Cov(educ, exp)}}{\hat{Var(educ)}}
\end{equation}

Y $\hat{\beta_{1}}'$ será insesgado solo sí $\beta_{2} = 0$ o si $cov(educ,exp)=0$

Dado lo anterior podemos preguntarnos ¿cuándo ocurre que $E[\hat{\beta_{1}}] = \beta_{1}$? tenemos dos opciones  $\beta_{2} = 0$ o si $cov(educ,exp)=0$

¿Se cumplirán estas condiciones en el ejemplo que hemos visto? 

1. Por un lado, $\beta_{2} \neq 0$ si la $exp$ determina $wages$ lo cual ocurre. 

2. Por otro lado,  $cov(educ,exp)\neq0$ dado que $exp$ y $educ$ están relacionados directamente. 

Dado que en este caso ninguna de las condiciones se cumplirán nuestro $\hat{\beta_{1}}'$ sufriá de sesgo por variable omitida  por lo que no va a converger a $\beta_{1}$. 

De tal forma, si el término de error está correlacionado con los regresores, entonces obtendremos estimdores sesgados. El término de error $u$ incluye todo lo que no hemos medido e incluido en la regresión, tal como variables omitidas. 

Si alguna de las variables omitidas: 

1. Influye en la variable depndiente ($\beta_{2} = 0$) y 
2. Está relacionada con los regresiores de la ecuación ($cov(X_{1}, X_{2}) \neq 0$) entonces nuestro MCO estará sesgado. 
En otras palabras, las propiedades del término de error determinarán si nuestro MCO es insesgado. 


### Ejemplo: Sesgo por Variable Omitida

Supongamos que tenemos un modelo verdadero dado por: 

\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + u_{i} ... (1)
\end{equation}

Y corremos el sigueinte modelo: 

\begin{equation}
Y_{i} = \beta_{0}' + \beta_{1'}X_{i1} + v_{i} ... (2)
\end{equation}

Y tenemos un modelo auxiliar: 

\begin{equation}
X_{i2} = \delta_{0}' + \delta_{1}'X_{i1} + \epsilon_{i} ... (3)
\end{equation}

En este caso simple donde tenemos un regresor y una variable omitida, al momento de estimar la ecuación (2) con MCO obtendremos: 

\begin{equation}
E(\hat{\beta_{1}}) = \beta_{1} + \beta_{2}\delta_{1}
\end{equation}

De manera equivalente, nuestro sesgo estría dado por: 

\begin{equation}
E(\hat{\beta_{1}}) - \beta_{1} = \beta_{2}\delta_{1}
\end{equation}

Podemos considerar los siguientes resultados: 


```{r, out.width="50%"}
knitr::include_graphics("Reg_Dif.png")
```

Notemos que para el modelo verdadero, el coeficiente de la educación $\beta_{1}$ es .093168, y el coeficiente de la experiencia $\beta_{2}$ es .0406574.

Al momento de correr el modelo incompleto (en la parte de arriba) tenderemos nuestro estimador $\beta_{1}'$ dada por el coeficiente dado por .0520942. 

En la parte de abajo tendremos nuestro modelo auxiliar, al momento de ver el coeficiente podemos notar que hay una correlación negativa entre experiencia y educación, mismo que será nuestro $\delta_{1}$.

Podemos calcular y confirmar que: \hat{\beta_{1}}' = \hat{\beta_{1}} + \hat{\beta_{2}}\hat{\delta_{1}}  = 0.0932 + 0.0407(-1.010) = 0.0521, mismo que corresponde a los resultados de correr el modelo incorrectamente especificado/sesgado. 

Lo anterior se reduce a lo siguiente: 


\begin{equation}
Sesgo = E(\hat{\beta_{1}}') - \beta_{1} = \beta_{2}\delta_{1}
\end{equation}


Cuando omitimos un segundo regresor ($X_{2}$) nos sesgará al estimador del coeficiente en el primer regresor ($X_{1}$) a menos que: 

1. El segundo regresor no tiene efecto en la variable dependiente ($\beta_{2} = 0$).
2. O bien el segundo regresor no está correlacionado con el primer regresor $(Cov(X_{1}, X_{2})=0)$ tal que $\delta_{1}=0$


## Dirección del Sesgo por Variable Omitida

\begin{equation}
Y = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + u
\end{equation}

Tabla: Sesgo en $\hat{\beta_{1}}'$ cuando 


|   |      $cov(X_{1},X_{2}) >0$    | $cov(X_{1},X_{2}) <0$   |
|----------|:-------------:|------:|
| $\beta_{2} > 0$ |  Sesgo Positivo | Sesgo Negativo |
| $\beta_{2} < 0$ |    Sesgo Negativo   | Sesgo Positivo |

Cuando tenemos un sesgo negativo quiere decir que estamos estimando por debajo del valor verdadero del estimador, por el otro lado, si el sesgo es positivo quiere decir que podemos estár sobre estimando por encima del valor verdadero del estimador. 

Más allá del caso simple: 

* La dirección del sesgo puede ser potencialmente más complicado si hay más de dos variables. 

* El sesgo en un coeficiente puede sesgar los demás, por lo que la variable omiita tiene que estár correlacionada únicamente con un rgresor para sesgar todos los estimadores. 

Por el lado optimiza, podemos "controlar por todo", dado que las variables omitidas pueden hacer que nuestro MCO esté sesgado, la idea principal de la regresión múltiple será que si incluimos las variables omitidas, podemos estimar el efecto de un regresor manteniendo lo demás constante. 

Del lado pesimista, nunca podremos sabero medir todas las variables relevantes, por lo que deberíamos regresar a "estrategias de identificación" donde tenemos alguna razón para creer que $E(u|X) =0$, por ejemplo un RCT. 


## Regresión multivariada 

Recordemos que: 

\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}X_{i1} + ... + \beta_{k}X_{ik} + u_{i}
\end{equation}


Donde $Y$ es la variable dependiente, variable de respuesta, variable predecida, regresandos. 


Por otro lado $X$ serán las variables independientes, variables de control, variables predictores, regresores, en algunos casos se llama a $X$ como "variable de interés", en otros se le llama "controles".

Por tanto, nuestro problema de optimización MCO será el siguiente:

\begin{equation}
min_{b_{0}, b_{1},...,b_{k}} \sum_{i=1}^n (Y_{i} - b_{0} - b_{1}X_{i1} - ... - b_{k}X_{ik})^2
\end{equation}

Podeemos agregar regresores, siempre y cuando se cumpla que $n>k+1$, es decir los parámetros no pueden exceder el número de variable sque tenemos, generalmente cuando tenemos muchos parámetros en nuestro modelo tenderá a hacer un overfit. 

Las condiciones de primer orden son análogas al caso bivariado, aunque las soluciones pueden ser complicada en la notación estándar, por lo que usaremos notción matricial. 

¿Cómo podemos interpretar la ecuación? 

\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}X_{i1} + ... + \beta_{k}X_{ik} + u_{i}
\end{equation}

* Un cambio en $X_{1}$ y $X_{2}$ en $Y$ será dado por:

\begin{equation}
\Delta Y_{i} = \beta_{1} \Delta X_{i1} + \beta_{2} \Delta X_{i2}
\end{equation}


* Si solo cambianmos $X_{1}$, entonces tendremos $\Delta X_{2} = 0$ y por tando: 


\begin{equation}
\Delta Y_{i} =  \beta_{1} \Delta X_{i1}
\end{equation}

Por lo que $\beta_{1}$ nos dirá el cambio en la esperanza condicional de $Y$ si aumentamos $X_{1}$ una unidad manteniendo $X_{2}$ constante. 

Así tomando 

\begin{equation}
\Delta Y_{i} =  \beta_{1} \Delta X_{i1}
\end{equation}

Y manteniendo todo lo demás constante, podemos notar que la interpretación es como en el caso bivariado, por tanto podemos definir que:

* $\beta_{1}$ es el efecto parcial de $X_{1}$ en $Y$.
* O bien, podemos decir uqe $\beta_{1}$ es la derivada parcial de $E[Y]$ con respecto a $X_{1}$.


Como ejemplo podemos pensar en


\begin{equation}
\hat{lwage} = 4.6660 + 0.0932educ + 0.0407exp
\end{equation}

Podríamos interpretar el resultado del modelo (sin usar lenguaje causal) de la sigueinte forma: 

* Manteniendo la experiencia constante, si aumentamos la educación en 1 año, esto aumenta el log wage esperado en .0932.

* Es decir, ceteris paribus, aumentar la educación en un año está asociado en un aumento del 0.0932 en el log wage. 


## Valores predecido y residuales. 

Los valores predecidos son análogos al caso bivariado

\begin{equation}
\hat{Y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}X_{i1} + ... + \hat{\beta_{k}}X_{ik} + u_{i} = E(\hat{Y|X})
\end{equation}

Donde $X$ es el vector: $E(Y|X) = E(Y|X_{1}, X_{2},..,X_{k})$

Los residuos son análogos al caso bivariado: 


\begin{equation}

\hat{u_{i}} = Y_{i} - \hat{Y_{i}}

\end{equation}

Podemos pensar en el caso de ejemplo que hemos manejado: 

* ¿Cuál será el salario predecido para alguien con 16 años de educación y 10 años de experiencia? 

\begin{equation}
\hat{lwage_{i}} = 4.6660 + 0.0932(16) + 0.0407(10) = 6.5642
\end{equation}

* ¿Qué pasaría si queremos agregar un año extra de educación manteniendo la experiencia fija? 

\begin{equation}
\hat{lwage_{i}} = 0.0932 \Delta educ
\end{equation}

\begin{equation}
\rightarrow \hat{lwage_{i}} = 6.5642 + 0.0932 = 6.6574
\end{equation}

* ¿Qué pasaría si agregamos dos años más de educación y disminuimos un año de experiencia? 

\begin{equation}
\hat{lwage_{i}} = 0.0932 \Delta educ + 0.0407 \Delta exp
\end{equation}

\begin{equation}
\rightarrow \hat{lwage_{i}} = 6.5642 + 0.0932(2) - 0.047 = 6.7099
\end{equation}


Sin embargo, queda preguntarnos: ¿A qué nos refereimos cuando decimos que mantenemos todo lo demás constante (ceteris paribus)?


* Woolridge (p.77) : "El poder del análisis en la regresión multivariada es que nos permite crear un ambiente no experimental, mismo que en las ciencias naturales es posible en un laboratorio controlado", lo anterior resulta particularmente útil cuando pensamos en el potencial de la regresión multivariada, aunque es necesario mencionar que dicha interpretación resulta errónea si la regresión no es causal. 



## Supuestos de Gauss - Markov 

Serán muy similares al caso bivariado: 

1. $E(u_{i}|X_{i1},...,X_{ik}) = 0$

2. No Multicolinerealidad perfecta. (No debemos incluir dos regresores que tengan la misma información)

3. ($X_{i1},...,X_{ik}, Y_{i}$) son i.i.d.

4. Los "outliers" son poco comunes (curtosis finita)

Estos cuatro supuestos implican que el M.C.O es insesgado, y también si $n$ es suficientemente larga, entonces $\hat{\beta}$ es distribuida aproximadamente normal. 

### Media condicional igual a 0

$E(u_{i}|X_{i1},...,X_{ik}) = 0$ implica que la media del error debe de ser cero para cualquier combinación de valores de los diferentes regresores. 

Por ejemplo, $E[u|educ = 6, exp = 10] =0, E[u|educ = 20, exp = 3] = 0$. 

Esta condicón será violada si cualquier regresos está correlacionad con el término de error $u$. 

También si tenemos alguna variable omitida, esto porque: 

1. Los valores faltantes afectan el resultado. 
2. Los valores faltantes están correlacionados con las variables incluidas. 

Por lo cual, aunque en los modelos multivariados tenemos la capacidad de controlar regresores adicionales, aún es posible que existan variables que afecten el resultado y estén relacionados con los regresores. 

No podemos interpretar los residuos como términos de error, esto debido a la propiedad numérica en la que los residuios están no correlacionados con cada nivel de $X$.


### Multicolinearidad Perfecta

Un conjunto de variables que será perfectamente multicolineal si hay una relación exacta entre el conjunto de variables, por ejemplo pensemos en : 

\begin{equation}
X_{j} = \delta_{0} + \delta_{1}X_{1}+...+\delta_{j-1}X_{j-1} + \delta_{j+1}X_{j+1} +...+ \delta_{k}X_{k}
\end{equation}

Con $R^2 = 1$

Ahora pensemos que tenemos $X_{2} = 2X_{1}$ y:

\begin{equation}
Y = 3X_{1} + 2X_{2}+u 
\end{equation}

Ahora bien, si predecimos $Y$ será exactamente igual a si tuviésemos: 

\begin{equation}
Y = 1X_{1} + 3X_{2}+u 
\end{equation}


Por lo que $X_{1}$ predice perfectamente a $X_{2}$, por lo que no podemos saber nada sobre los efectos separados de $X_{1}$ y $X_2$ por lo que no tendremos información nueva. 


Algunos ejemplos de multicolinealidad perfecta: 

1. Si $X_{2} =aX_{1}$ para cualquier a.
2. Una variable dummy para $mujer$ en una muestra de mujeres. 
3. experiencia = edad - años de educación - 6


## Ejemplo de Regresión multivariada 

* Neal y Johnson (1996) estudiaron la diferencia en salarios entre gente blanca y negra, esto con motivo de saber: 

1. Si hay alguna discriminación en el mercado laboral. 
2. Preocupación dado que la discriminación genera una experiencia, 


* Se usaron datos de NLSY en donde se miden las habilidades al momento de entrada al mercado laboral, algunas de las preguntas relacionadas a la regresión fueron: 

1. ¿Cómo puedo determinar la diferencia salarial con MCO?
2. ¿Cómo podemos interpretar los coeficientes? 
3. ¿Debería de controlar por muestra o edad?
4. ¿Cómo puedo controlar por género?
5. ¿Debería interpretar la regresión de forma causal?


```{r, out.width="50%"}
knitr::include_graphics("Labor_Reg.png")
```

Podemos observar que tenemos regresiones sobre la variable log(wage), la transformación logarítmica regularmente se hace para restarle peso a outliers sin afectar la naturaleza de su media y varianza dado que es una transformación lineal, podemos observar que se separan los efectos entre mujeres y hombres, en la columna de la izquierda vemos al segmento de hombres, también notemos que tenemos tres segmentos dentro de la muestra clasificados como individuos hispanos, afrodescendientes y el resto (que podemos identificar mayoritariamente como blancos y asiáticos), éstos serán categorizados a través de variable dummy (que tomarán. valores de 0 y 1). 

Podemos observar que hay una diferencia de -.244 entre hombres negros y negros blancos, las cantidades que se encuentran en el paréntesis corresponden a los errores estándar, los cuales nos sirven para establecer la precisión estadística o la signifcancia estadística calculando el estadístico t mismo que será $\frac{\beta_{1} - \beta_{hipótesis \ nula}}{error \ estándar(\beta_{1})}$, en este caso sería $\frac{.244}{.026}$ aunque notemos que el $ \beta_{hipótesis \ nula}$ se omitió dado que esté será igual a 0, generalmente nos preguntaremos si este valor es mayor a 2, y que para este caso es 9.384, por lo que será mayor a 2 entonces será áltamente estadísticamente preciso.

La columna 2 se refiere a educación por lo que podemos ver que la diferencia entre hombres negros y blancos será ahora de -.196, en el caso hispano se reduce casi a la mitad de -.113 a -.045, después utilizan la variable AFQT que es un test para medir habilidades, y se comparan con la columna 3. 

En la columna 4 podemos ver la diferencia por género, notando que también las mujeres afrodescendientes reciben un menor ingreso en una magnitud de -.185, también podemos ver que una vez que condicionamos a habilidades hay relativamente poca discriminación, por lo que las minorias racionales probablemente experimentan poca discriminación antes de entrar al mercado laboral. 

Algo también importante a ver es la $R^2$ la cual se define como la variación que puede ser explicada de nuestro resultado $Y$ por nuestro modelo en cuestión, si solo consideramos edad y raza podemos explicar el $.059$ o bien aproximadamente $6$% de la variación, mientras que si agregamos educación se explica $15$%, en el caso de mujeres la edad y la raza explica aproximadamente $3$% de la variación, mientras que la educación es $19.1$% y si agregamos el test AFQT explicará $16.5$%


## Fit de la Regresión multivariada 

Tendremos una $R^2$ multivariada la cual será análoga al caso bivariado: 

\begin{equation}

R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}

\end{equation}

Recordemos que: 

* ESS será la suma de los cuadrados explicados. 

* SSR será la suma de los residuales al cuadrado. 

* TSS será la suma de los cuadrados. 

Al momento de comparar la $R^2$ podemos saber cuál modelo será mejor para axplicar la variación de $Y$. No obstante, habrá un problema en usar esta métrica en MCO multivariado lo cual se deberá a que el regresor $R^2$ nunca bajará cuando agregamos un regresor, dado que agregar regresores irrelevantes no puede hacer que el SSR aumente (agregar regresores no puede hacer que la optimización de MCO sea peor).

### $R^2$ Ajustada

Para solucionar el problema anterior, usaremos $R^2$ ajustada, la cual considera el número de regresores $X$, $k$ tales que: 

\begin{equation}
adj - R^2 : 1 - \frac{(n-1)}{(n-k-1)} \frac{SSR}{TSS}
\end{equation}

Por lo que añadir un regresor disminue $SSR$ pero también aumenta la diferencia $\frac{n-1}{n-k-1}$ como si estuviera "penalizandolo".Así, agregar un regresor solo aumenta $\bar{R^2}$ si tiene "suficiente" poder explicativo.

También notemos que: $\frac{n-1}{n-k-1} > 1$ entonces $R^2 >$ adj-$R^2$, y también adj-$R^2$ estará acotado superiormente por 1 como la $R^2$ pero también podrá ser negativo, esta métrica nos ayudará a justificar el agregar un regresor a nuestro modelo. 


## MCO como forma de parcialización: Teorema de Frisch - Waugh - Lovell 

Consideremos el modelo multivariado estándar:

\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + u_{i}
\end{equation}


Recordemos que: 

* $\beta_{1}$ nos dice la asociación entre $X_{i1}$ y $Y_{i}$ manteniendo $X_{i2}$ fija.

* Podemos parcializar $X_{i2}$ de la siguiente forma: 

1. Correr regresión de $X_{i1}$ sobre $X_{i2}$, obteniendo los residuales: $\hat{X_{i1}}^r = X_{i1} - \hat{\delta_{0}} -  \hat{\delta_{1}}X_{i2} $  

2. Correr regresión de $Y_{i}$ sobre $X_{i2}$, obteniendo los residuales: $\hat{Y_{i}}^r = Y_{i1} - \hat{\alpha_{0}} -  \hat{\alpha_{1}}X_{i2}$ 

3. Correr regresión de $\hat{Y^r}$ sobre  $X_{i1}^r$, obtener $\hat{\beta_{1}}$

Lo anterior dado que recordemos que los residuales son la parte de la variación que no está explicada por los regresores de un modelo, de tal forma para 1. obtendriamos la variación en$X_{i1}$ que no está en $X_{i2}$, es decir la variación de $X_{i1}$ que es independiente de $X_{i2}$, en 2. vemos la variación en el resultado que es independiente de$X_{i2}$, podemos tomar cualquiera de los residuos y usarlos para recuperar el estimador original, al final lo que estaríamos identificando $\hat{Y^r}$ sobre  $X_{i1}^r$ será lo que no está influenciado por $X_{i2}$ lo cual es lo mismo que si mantuviesemos $X_{i2}$  fijo.



```{r, out.width="50%"}
knitr::include_graphics("Tabla_par.png")
```
En este caso se corrió una regresión con $log(wage)$ en función de $educ$ y $exp$, notemos que tenemos nuestro coeficiente de educación el cual será .107, ahora podemos pensar en la variación de los residuos, misma que corresponderá a la parte de $log(wage)$ que no está predecido por la variación en $exp$, obtendremos lo anterior con los comandos de STATA: quietly regress lwage exper, y vamos a predecir los residuos con el comando predict lwage_resid, haremos lo mismo para $educ$ y después correremos la regresión usando los residuos con regress lwage_resid educ_resid, noheader, y obtendremos el mismo resultado que tenemos cuando predecimos $educ$ y dejamos fija $exp$, notemos que los errores estándar cambian lo cual involucra un paso extra pero eso no será cubierto. 

## MCO multivariado: Precisión e inferencia. 

La multicolinearlidad imperfecta se refiere a que hay una o más variables que están altamente correlacionadas, es importante notar que la mulitcolinearidad no sesga los estimadores pero sí les resta precisión, dado que con regresores correlacionados resulta difícil para MCO determinar cuáles variables responden más a los cambios en Y, cuando parcializamos los residuos de la regresión de $X_{1}$ sobre $X_{2}$ son menores, estos residuos se convierten en la regresión en el siguiente paso, por lo que menor varianza en el regresor $\rightarrow$ errores estándar más grandes. 

Ahora pensemos en la fórmula de $\hat{\beta}$, recordemos que estamos interesados en $\beta_{1}$ en nuestra ecuación:

\begin{equation}

Y_{i} = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + u_{i}

\end{equation}

Podemos parcializar obteniendo: 

\begin{equation}

\hat{Y_{i}}^r = \beta_{0} + \hat{\beta_{1}X_{i1}}^r + u_{i}

\end{equation}

Y expandiendo podemos obtener:

\begin{equation}

\hat{\beta_{1}} = \frac{\sum_{i=1}^n(\hat{X_{i1}^r}- \bar{\hat{X_{i1}}^r}) (\hat{Y_{i1}^r}- \bar{\hat{Y_{i1}}^r})}{\sum_{i=1}^n (\hat{X_{i1}^r}- \bar{\hat{X_{i1}}^r})^2} = \frac{\sum_{i=1}^n \hat{X_{i1}^r} \hat{Y_{i1}^r}}{\sum_{i=1}^n \hat{X_{i1}^r}^2}

\end{equation}


Ahora bien, para la varianza de $\hat{\beta}$ como en el caso bivariado, si $n$ es grande, entonces los supuestos MCO se cumplen, y por tanto $\hat{\beta}$ se distribuye de forma normal, por lo tanto podemos escribir la varianza de $\hat{\beta}$ manteniendo el supuesto de homoscedasticidad: 

\begin{equation}
\sigma_{\hat{\beta_{1}}}^2 = \frac{\sigma_{u}^2}{ (1- R_{1}^2) \sigma_{i=1}^n (X_{i1} - \bar{X_{1}}) ^2} = \frac{1}{n} \frac{\sigma_{u}^2}{(1-R_{1}^2)\sigma_{X_{1}}^2 }
\end{equation}


Donde $R_{1}^2$ es la $R_{1}^2$ de la regresión de $X_{1}$ sobre todas las variables de $X$, lo cual significa que en la regresión multivariada tenemos que pensar en diferentes factores, uno de ellos es el tamaño de la muestra (recordemos que si el tamaño de la muestra aumenta entonces la $\sigma_{\hat{\beta_{1}}}^2$ aumentará en precisión, también notemos que $\sigma_{u}^2$ será la varianza del término de error y recordemos que si la varianza del término de error disminuye entonces toda la expresión disminuye, también $\sigma_{X_{1}}^2$ recordemos que queremos tener mayor varianza den $X_{1}$ como sea posible pero tenemos que ser cuidadosos dado que ahora $X_{1}$ no será el único factor que estará en nuestro modelo por lo que tenemos que pensar en otros factores que podrían estar correlacionados potencialmente con $X_{1}$, recordemos que estamos identificando el valor de $X_{1}$ que es independiente de $X_{2}$ por lo que si hay un componente sustancial en $X_{1}$ que se explica por $X_{2}$ entonces eso disminuirá la varianza para explicar y por tanto la precisión. 

También veremos que $(1-R_{1}^2)$ será la regresión de $X_{1}$ sobre todas las otras variables, es decir la cantidad de $X_{1}$ que se explica por los otros regresores de nuestro modelo, si la cantidad de ese variación es grande entonces la varianza disminnuirá. 

Lo anterior funciona dado que tendremos:

\begin{equation}

\sigma_{\hat{\beta_{1}}}^2 = \frac{1}{n} \frac{\sigma_{u}^2}{(1-R_{1}^2) \sigma_{X_{1}}^2)}
\end{equation}

E intuitvamente podemos decir que:

* $var(\hat{\beta_{1}})$ aumenta con $\sigma_{u}^2$

* $var(\hat{\beta_{1}})$ disminuye con la varianza de $X_{1}$

* $var(\hat{\beta_{1}})$ aumenta con la cantidad de $X_{1}$ que se explica por otros regresores. 


Así, la inferencia será casi la misma que en el caso bivariado:

* Estimar $\sigma_{\beta_{1}}$ sustituyendo los valores estimados. 

* Dado que $\hat{\beta_{j}}$ se distribuye normalmente, el estadístico t será: $\frac{\hat{\beta_{j}} -\beta_{j}} {\hat{\sigma_{\hat{\beta_{1}}}^2}}$ distribuudi t con $n-k-1$ grados de libertad y convergirán cuando el grado de libertad sea aproximadamente 100. 

* El valor $p$ será aproximadamente $2\Phi(-|t|)$ y los intervalos de confianza serán construidos de la misma manera que en el caso bivariado. 

Por tanto $R^2$ no disminuirá si agregamos más regresores, por lo que $R^2$ será menos valiosa al momento de comparar con otros modelos, usaremos adj$-R^2$ en su lugar. También el modelo MCO multivariado puede ser intrepretado como si estuviésemos parcializando los efectos de otras variables y agregar un regresor extra (algo no relacionado con $Y$) no va a sesgar los otros regresores pero va a disminuir la precisión. 


## Pruebas de hipótesis y test F

Imaginemos que tenemos estimadores para $\beta$ y $\hat{\sigma_{\hat{\beta}}}$ y queremos probar la hipótesis de que $\beta = \theta$, es decir: $H_{0} : \beta =0$. 

El estadístico de prueba $\frac{\hat{\beta}-\theta}{\hat\sigma_{\hat{\beta}}}$ estará distribuido t con $n-k-1$ grados de libertad. 


```{r, out.width="50%"}
knitr::include_graphics("dis_t.png")
```

La forma en la que lo haremos será muy similar a los casos anteriores, comparando contra uns distribución en particular, la línea punteada representará la distribución normal, la línea sólida será la distribución t y vamos a rechazar la hipótesis si $|t|>1.96$. 


Ahora bien, si queremos testear con respecto a diferentes restricciones usaremos el estadístico de prueba F, supongamos por ejemplo que queremos testear si $\beta_{jc} = \beta_{univ}$ y también que $\beta_{exper} = 0.006$, de tal forma, podemos imaginar los siguients test de forma: 

*$H_{0}: \beta_{j} = \theta_{1}, \beta_{m} = \theta_{2}..$

*$H_{A}: \beta_{j} \neq \theta_{1}, \beta_{m} \neq \theta_{2}..$

donde el test siempre será que todas las condicones nulas se mantengan, necesitaremos un test $F$ para esto.


Así, consideremos el resultado de la siguiente regresión cuando predecimos los salarios de ligas mayores de baseball:

\begin{equation}


log(salary) = \underset{(0.29)}{11.19}+\underset{(0.01121)}{0.0689 years} + \underset{(0.0026)}{0.0126gamesyr} + \underset{(0.00110)}{0.00098bavg} + \underset{(0.0161)}{0.01448hrunsyr} + \underset{(0.0072)}{0.0108sbisyr} 

\end{equation}

con $n=353$, $SSR=183.18$, $R^2=0.6278$ por lo que podremos explicar aproximadamente .63% de la variación del salario con el modelo propuesto, al momento de calcular el estadístico t para $years = 0.0689/0.0121 = 5.6942$ será mayor a 2 y por tanto estadísticamente significativo, para $gamesyr = 0.0126/0.0026 = 4.846$ será también mayor a 2 por lo que será estadísticamente significativo pero para$bavg = 0.00098/0.00110 = .890$, $hrunyr = 0.0144/0.0161 = 0.894$ y $rbisyr 0.0108/.0072 = 1.5$ podemos ver que no son determinantes importantes del salario dado que su estadístico t no es significativo (menor a 2) ¿podríamos decir que estos valores no son relevantes para determinar el salario? no necesariamente, dado que parece ser el caso que no es que sean determinantes importantes sino que cada uno por sí mismo no es estadísticamente preciso. 

Ahora bien, podríamos pensar en la hipótesis nula como una restricción y estimar el MCO imponiendo la hipótesis nula (restringimos MCO para que solo eliga ciertos valores de $\hat{\beta}$), así: 

* Si la restricción es verdadera, entonces imponer los valores para $\hat{\beta}$ no va a limitar la habilidad del modelo para que se ajuste a los datos por mucho. 
* Si la restricción es falsa, el modelo tendrá un rendimiento bajo. 

El procedimiento de lo anterior es: 

* Correr el modelo no restringido 
* Correr el modelo restringido 
* Comparar el SSR

Rechazar la hipótesis nula si al momento de imponerla causa que el SSR aumente por "mucho". También el test F sigue ese procedimiento y nos dice que tanto es "mucho".

Dado lo anterior al momento de testear la hipótesis conjunta de que $\beta_{bavg}=\beta_{hrunsyr} = \beta_{rbisyr = 0}$ corremos la regresión auxiliar que impone la restricción hipotética:

\begin{equation}
log(salary) =  \underset{(0.11)}{11.22} + \underset{(0.0125)}{0.0713years} + \underset{(0.0013)}{0.0202gamesyr}
\end{equation}

con $n=353$ , $SSR=198.311$, $R^2 =0.5971$ y supongamos que $bavg$, $hrunsyr$ y $rbisyr$ no son importantes, por lo que si las removemos del modelo debería causar que el SSR aumente solo un poco, el estadístico F será un test más apropiado para esta intuición. 





```{r cars}

library(ggplot2)
library(NHANES)
library(tidyverse)

colnames(NHANES)

# Create bar plot for Home Ownership by Gender
ggplot(NHANES, aes(x = Gender, fill = HomeOwn)) + 
  # Set the position to fill
  geom_bar(position = "fill") +
  ylab("Relative frequencies")

# Density plot of SleepHrsNight colored by SleepTrouble
ggplot(NHANES, aes(x = SleepHrsNight, color = SleepTrouble)) + 
  # Adjust by 2
  geom_density(adjust = 2) + 
  # Facet by HealthGen
  facet_wrap(~ HealthGen)


#Using the NHANES dataset, let's investigate the relationship between gender and home ownership. Remember, more information #about the dataset can be found here: NHANES.

# Subset the data: homes
homes <- NHANES %>%
  # Select Gender and HomeOwn
  select(Gender, HomeOwn) %>%
  # Filter for HomeOwn equal to "Own" or "Rent"
  filter(HomeOwn %in% c("Own", "Rent"))
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
